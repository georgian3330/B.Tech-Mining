{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.) What is feature engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering is the process of selecting, transforming, or creating features (input variables) from raw data to improve the performance of machine learning models. It involves techniques to make the data more suitable for the model by enhancing its predictive power.\n",
    "\n",
    "Key steps in feature engineering include:\n",
    "\n",
    "1. Feature Selection: Choosing the most relevant features from the dataset.\n",
    "2. Feature Transformation: Applying mathematical or statistical transformations (e.g., normalization, scaling, or encoding categorical variables).\n",
    "3. Feature Creation: Generating new features from existing ones (e.g., combining features, extracting date/time components, or creating interaction terms).\n",
    "\n",
    "Effective feature engineering can significantly improve model accuracy and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.) Explain the Imputation, Handling Outliers, Log Transform, One-Hot Encoding, Feature \n",
    "Split, and Scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Imputation\n",
    "* Purpose: To handle missing data in a dataset.\n",
    "* How it works:\n",
    "    * Replace missing values with a statistical measure like the mean, median, or mode.\n",
    "    * Use advanced techniques like K-Nearest Neighbors (KNN) imputation or regression-based imputation.\n",
    "* Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')  # Replace missing values with the mean\n",
    "data = \"data.csv\"\n",
    "data_imputed = imputer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Handling Outliers\n",
    "* Purpose: To manage extreme values that can skew the model's performance.\n",
    "* How it works:\n",
    "    * Detect outliers using methods like the Interquartile Range (IQR), Z-score, or visualization (boxplots).\n",
    "    * Handle outliers by capping, removing, or transforming them.\n",
    "* Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using IQR to remove outliers\n",
    "Q1 = data['feature'].quantile(0.25)\n",
    "Q3 = data['feature'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "data_filtered = data[(data['feature'] >= Q1 - 1.5 * IQR) & (data['feature'] <= Q3 + 1.5 * IQR)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Log Transform\n",
    "* Purpose: To reduce skewness in data and make distributions more normal-like.\n",
    "* How it works:\n",
    "    * Apply a logarithmic transformation to features with highly skewed distributions.\n",
    "    * Commonly used for features with large ranges or exponential growth.\n",
    "* Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data['log_feature'] = np.log1p(data['feature'])  # log1p handles log(0) by adding 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. One-Hot Encoding\n",
    "* Purpose: To convert categorical variables into a numerical format suitable for machine learning models.\n",
    "* How it works:\n",
    "    * Create binary columns (0 or 1) for each category in a categorical feature.\n",
    "* Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_data = encoder.fit_transform(data[['categorical_feature']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Feature Split\n",
    "* Purpose: To split a single feature into multiple components for better representation.\n",
    "* How it works:\n",
    "    * Split features like date/time into components (e.g., year, month, day).\n",
    "    * Split text data into tokens or substrings.\n",
    "* Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting a date column\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Scaling\n",
    "* Purpose: To standardize the range of features to ensure all features contribute equally to the model.\n",
    "* How it works:\n",
    "    * Apply techniques like Min-Max Scaling (normalization) or Standard Scaling (z-score normalization).\n",
    "* Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.) Identify  and  handle  missing  values  in  the  House  Prices  Dataset  using  appropriate  \n",
    "imputation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle missing values in the House Prices Dataset, we can use appropriate imputation techniques. Here, we will use the mean for numerical columns and the mode for categorical columns.\n",
    "\n",
    "Here is an example of how to do this using Python and the pandas library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Price    Area     Location  Number_of_Rooms  Year_Built\n",
      "0  250000.0  1200.0  City Center             3.00     2001.00\n",
      "1  300000.0  1500.0      Suburbs             4.00     1999.00\n",
      "2  375000.0  1800.0         None             3.00     2005.00\n",
      "3  450000.0  1625.0      Suburbs             5.00     2003.75\n",
      "4  500000.0  2000.0  City Center             3.75     2010.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Price': [250000.0, 300000.0, None, 450000.0, 500000.0],\n",
    "    'Area': [1200.0, 1500.0, 1800.0, None, 2000.0],\n",
    "    'Location': ['City Center', 'Suburbs', None, 'Suburbs', 'City Center'],\n",
    "    'Number_of_Rooms': [3.0, 4.0, 3.0, 5.0, None],\n",
    "    'Year_Built': [2001.0, 1999.0, 2005.0, None, 2010.0]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Impute missing values for numerical columns with the mean\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "df[['Price','Area', 'Number_of_Rooms', 'Year_Built']] = num_imputer.fit_transform(df[['Price', 'Area', 'Number_of_Rooms', 'Year_Built']])\n",
    "\n",
    "# Impute missing values for categorical columns with the mode\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[['Location']] = cat_imputer.fit_transform(df[['Location']])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.) Apply feature scaling (Min-Max Scaling and Standardization) to the Student \n",
    "Performance Dataset and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply feature scaling to the Student Performance Dataset, we will use both Min-Max Scaling and Standardization. We will then compare the results.\n",
    "\n",
    "Here is an example using Python and the pandas and sklearn libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   Math_Score  Reading_Score  Writing_Score  Attendance_Percentage\n",
      "0          80             78             75                     90\n",
      "1          90             88             85                     95\n",
      "2          70             68             65                     85\n",
      "3          65             72             70                     80\n",
      "4          85             82             80                     92\n",
      "\n",
      "Min-Max Scaled Data:\n",
      "   Math_Score  Reading_Score  Writing_Score  Attendance_Percentage\n",
      "0         0.6            0.5           0.50               0.666667\n",
      "1         1.0            1.0           1.00               1.000000\n",
      "2         0.2            0.0           0.00               0.333333\n",
      "3         0.0            0.2           0.25               0.000000\n",
      "4         0.8            0.7           0.75               0.800000\n",
      "\n",
      "Standardized Data:\n",
      "   Math_Score  Reading_Score  Writing_Score  Attendance_Percentage\n",
      "0    0.215666       0.056433       0.000000               0.301084\n",
      "1    1.293993       1.467265       1.414214               1.241971\n",
      "2   -0.862662      -1.354398      -1.414214              -0.639803\n",
      "3   -1.401826      -0.790066      -0.707107              -1.580691\n",
      "4    0.754829       0.620766       0.707107               0.677439\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Math_Score': [80, 90, 70, 65, 85],\n",
    "    'Reading_Score': [78, 88, 68, 72, 82],\n",
    "    'Writing_Score': [75, 85, 65, 70, 80],\n",
    "    'Attendance_Percentage': [90, 95, 85, 80, 92]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Min-Max Scaling\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df_min_max_scaled = pd.DataFrame(min_max_scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Apply Standardization\n",
    "standard_scaler = StandardScaler()\n",
    "df_standard_scaled = pd.DataFrame(standard_scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Display the results\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "print(\"\\nMin-Max Scaled Data:\")\n",
    "print(df_min_max_scaled)\n",
    "print(\"\\nStandardized Data:\")\n",
    "print(df_standard_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison:\n",
    "* Min-Max Scaling: Transforms the data to a fixed range, typically [0, 1]. This is useful when you want to ensure that all features contribute equally to the model.\n",
    "* Standardization: Transforms the data to have a mean of 0 and a standard deviation of 1. This is useful when the features have different units or scales, and you want to normalize them to a common scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
