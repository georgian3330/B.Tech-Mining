{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction (using PCA) and modelling (K-means) of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load MNISTdataset (handwritten digits)\n",
    "mnist = fetch_openml(('mnist_784'), version=1, as_frame=False)\n",
    "X = mnist.data[:5000] # Use a subset for faster processing\n",
    "\n",
    "# Apply PCA to reduce dimensions from 784 to 100\n",
    "pca = PCA(n_components = 100)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "# Percentage of variance retained\n",
    "print(\"Explained Variance Ratio:\", np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "\n",
    "# Reconstruct images from reduced PCA components\n",
    "X_reconstructed = pca.inverse_transform(X_pca)\n",
    "\n",
    "\n",
    "# Display original and reconstructed images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(X[0].reshape(28, 28), cmap ='gray')\n",
    "axes[0].set_title(\"Original Image\")\n",
    "\n",
    "# Reconstructed image using 100 PCA components\n",
    "axes[1].imshow(X_reconstructed[0].reshape(28, 28), cmap ='gray')\n",
    "axes[1].set_title(\"Reduced Image (PCA)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "\n",
    "# Load Olivetti Faces dataset (images of faces)\n",
    "faces = fetch_olivetti_faces(shuffle=True, random_state=420)\n",
    "X = faces.data # Each image is converted into a 1D array of pixel values\n",
    "\n",
    "\n",
    "# Apply PCA to reduce dimensions\n",
    "pca = PCA(n_components=100) # Reduce from 4096 features to 100\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "# Percentage of variance retained\n",
    "print(\"Explained Variance Ratio:\", np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "\n",
    "# Visualize original and reconstructed images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(X[0].reshape(64, 64), cmap='gray')\n",
    "axes[0].set_title(\"Original Image\")\n",
    "\n",
    "\n",
    "# Reconstructed image using 100 PCA components\n",
    "reconstructed = pca.inverse_transform(X_pca)\n",
    "axes[1].imshow(reconstructed[0].reshape(64, 64), cmap='gray')\n",
    "axes[1].set_title(\"Reduced Image (PCA)\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Simulation Traffic Data \n",
    "np.random.seed(42)\n",
    "traffic_data = pd.DataFrame({\n",
    "    \"Vehicle Count\" : np.random.randint(100, 500, 10),\n",
    "    \"Avg Speed\" : np.random.randint(30, 80, 10),\n",
    "    \"Time of Day\" : np.random.randint(0, 24, 10),\n",
    "    \"Road Type\" : np.random.randint(1, 5, 10)\n",
    "})\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "traffic_data_scaled = scaler.fit_transform(traffic_data)\n",
    "\n",
    "# Apply PCA to reduce dimensions\n",
    "pca = PCA(n_components=2)\n",
    "traffic_data_pca = pca.fit_transform(traffic_data_scaled)\n",
    "\n",
    "# Print variance explained \n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.DataFrame({\n",
    "    'Size' : [2000, 1500, 3000],\n",
    "    'Bedrooms' : [3, 2, 4],\n",
    "    'Bathrooms' : [2, 1, 3],\n",
    "    'Location Score' : [8, 7, 9],\n",
    "    'Year Built' : [2010, 2015, 2005]\n",
    "})\n",
    "\n",
    "# Standardize features (mean = 0, variance = 1)\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA: Reduce from 5 dimensions to 2\n",
    "pca = PCA(n_components = 2)\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Convert to DataFrame\n",
    "data_pca = pd.DataFrame(data_pca, columns = ['PC1', 'PC2'])\n",
    "\n",
    "# Print transformed data\n",
    "print(data_pca)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot of the reduced features\n",
    "plt.scatter(data_pca['PC1'], data_pca['PC2'], color = 'blue')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Dimensionality Reduction using PCA')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
